{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNngcd8UAOma"
   },
   "source": [
    "## Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8kAC3PZmiPPo",
    "outputId": "67b1ce13-5ac3-4269-8d34-c5201f32d77a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting open-clip-torch\n",
      "  Downloading open_clip_torch-2.31.0-py3-none-any.whl.metadata (31 kB)\n",
      "Requirement already satisfied: torch>=1.9.0 in /opt/miniconda3/lib/python3.11/site-packages (from open-clip-torch) (2.5.1+cpu)\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/lib/python3.11/site-packages (from open-clip-torch) (0.20.1+cpu)\n",
      "Requirement already satisfied: regex in /opt/miniconda3/lib/python3.11/site-packages (from open-clip-torch) (2024.11.6)\n",
      "Collecting ftfy (from open-clip-torch)\n",
      "  Downloading ftfy-6.3.1-py3-none-any.whl.metadata (7.3 kB)\n",
      "Requirement already satisfied: tqdm in /opt/miniconda3/lib/python3.11/site-packages (from open-clip-torch) (4.67.1)\n",
      "Requirement already satisfied: huggingface-hub in /opt/miniconda3/lib/python3.11/site-packages (from open-clip-torch) (0.26.5)\n",
      "Requirement already satisfied: safetensors in /opt/miniconda3/lib/python3.11/site-packages (from open-clip-torch) (0.4.5)\n",
      "Collecting timm (from open-clip-torch)\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: filelock in /opt/miniconda3/lib/python3.11/site-packages (from torch>=1.9.0->open-clip-torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/miniconda3/lib/python3.11/site-packages (from torch>=1.9.0->open-clip-torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/miniconda3/lib/python3.11/site-packages (from torch>=1.9.0->open-clip-torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/miniconda3/lib/python3.11/site-packages (from torch>=1.9.0->open-clip-torch) (3.1.5)\n",
      "Requirement already satisfied: fsspec in /opt/miniconda3/lib/python3.11/site-packages (from torch>=1.9.0->open-clip-torch) (2024.9.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/miniconda3/lib/python3.11/site-packages (from torch>=1.9.0->open-clip-torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/miniconda3/lib/python3.11/site-packages (from sympy==1.13.1->torch>=1.9.0->open-clip-torch) (1.3.0)\n",
      "Requirement already satisfied: wcwidth in /opt/miniconda3/lib/python3.11/site-packages (from ftfy->open-clip-torch) (0.2.13)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/miniconda3/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/miniconda3/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/miniconda3/lib/python3.11/site-packages (from huggingface-hub->open-clip-torch) (2.32.3)\n",
      "Requirement already satisfied: numpy in /home/luyaoton/.local/lib/python3.11/site-packages (from torchvision->open-clip-torch) (1.26.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/miniconda3/lib/python3.11/site-packages (from torchvision->open-clip-torch) (10.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/miniconda3/lib/python3.11/site-packages (from jinja2->torch>=1.9.0->open-clip-torch) (3.0.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch) (1.26.19)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/miniconda3/lib/python3.11/site-packages (from requests->huggingface-hub->open-clip-torch) (2024.12.14)\n",
      "Downloading open_clip_torch-2.31.0-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ftfy-6.3.1-py3-none-any.whl (44 kB)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m70.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "Installing collected packages: ftfy, timm, open-clip-torch\n",
      "\u001b[33m  WARNING: The script ftfy is installed in '/home/luyaoton/.local/bin' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\u001b[0m\u001b[33m\n",
      "Successfully installed ftfy-6.3.1 open-clip-torch-2.31.0 timm-1.0.15\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install open-clip-torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "eHNMKf0DlYTw"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2562133/90848741.py:23: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  s = torch.load(path_to_model)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df06fd05f5c14dddaa316ed7d7b9523e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "open_clip_model.safetensors:   0%|          | 0.00/1.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/luyaoton/.local/lib/python3.11/site-packages/open_clip/factory.py:388: UserWarning: These pretrained weights were trained with QuickGELU activation but the model config does not have that enabled. Consider using a model config with a \"-quickgelu\" suffix or enable with a flag.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from os.path import expanduser  # pylint: disable=import-outside-toplevel\n",
    "from urllib.request import urlretrieve  # pylint: disable=import-outside-toplevel\n",
    "def get_aesthetic_model(clip_model=\"vit_l_14\"):\n",
    "    \"\"\"load the aethetic model\"\"\"\n",
    "    home = expanduser(\"~\")\n",
    "    cache_folder = home + \"/.cache/emb_reader\"\n",
    "    path_to_model = cache_folder + \"/sa_0_4_\"+clip_model+\"_linear.pth\"\n",
    "    if not os.path.exists(path_to_model):\n",
    "        os.makedirs(cache_folder, exist_ok=True)\n",
    "        url_model = (\n",
    "            \"https://github.com/LAION-AI/aesthetic-predictor/blob/main/sa_0_4_\"+clip_model+\"_linear.pth?raw=true\"\n",
    "        )\n",
    "        urlretrieve(url_model, path_to_model)\n",
    "    if clip_model == \"vit_l_14\":\n",
    "        m = nn.Linear(768, 1)\n",
    "    elif clip_model == \"vit_b_32\":\n",
    "        m = nn.Linear(512, 1)\n",
    "    else:\n",
    "        raise ValueError()\n",
    "    s = torch.load(path_to_model)\n",
    "    m.load_state_dict(s)\n",
    "    m.eval()\n",
    "    return m\n",
    "\n",
    "amodel= get_aesthetic_model(clip_model=\"vit_l_14\")\n",
    "amodel.eval()\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBu_5UDhAREh"
   },
   "source": [
    "## Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fek7w2fSAKPD",
    "outputId": "b812aadc-ad5c-4b5e-c2d7-93b186e66038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2025-03-18 17:44:49--  https://thumbs.dreamstime.com/b/lovely-cat-as-domestic-animal-view-pictures-182393057.jpg\n",
      "146.75.9.91humbs.dreamstime.com (thumbs.dreamstime.com)... \n",
      "Connecting to thumbs.dreamstime.com (thumbs.dreamstime.com)|146.75.9.91|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 96578 (94K) [image/jpeg]\n",
      "Saving to: ‘lovely-cat-as-domestic-animal-view-pictures-182393057.jpg’\n",
      "\n",
      "lovely-cat-as-domes 100%[===================>]  94.31K  --.-KB/s    in 0.005s  \n",
      "\n",
      "2025-03-18 17:44:49 (19.0 MB/s) - ‘lovely-cat-as-domestic-animal-view-pictures-182393057.jpg’ saved [96578/96578]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!wget https://thumbs.dreamstime.com/b/lovely-cat-as-domestic-animal-view-pictures-182393057.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pNAZM7rBovU2",
    "outputId": "cac7f214-ac8a-4615-ced9-b6385cc833b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.4425]])\n"
     ]
    }
   ],
   "source": [
    "image = preprocess(Image.open(\"lovely-cat-as-domestic-animal-view-pictures-182393057.jpg\")).unsqueeze(0)\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    prediction = amodel(image_features)\n",
    "    print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "\n",
    "# Load models (assuming these are already initialized as in your notebook)\n",
    "# If needed, uncomment and run these lines:\n",
    "# model, _, preprocess = open_clip.create_model_and_transforms('ViT-L-14', pretrained='openai')\n",
    "# amodel = get_aesthetic_model(clip_model=\"vit_l_14\")\n",
    "# amodel.eval()\n",
    "\n",
    "def get_aesthetic_score(image_path):\n",
    "    try:\n",
    "        # Load and preprocess image\n",
    "        image = preprocess(Image.open(image_path)).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Extract features and normalize\n",
    "            image_features = model.encode_image(image)\n",
    "            image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Get aesthetic prediction\n",
    "            prediction = amodel(image_features)\n",
    "            return prediction.item()\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {image_path}: {e}\")\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SECTION 1: Aesthetic Scores for Left/Right Image Sets\n",
      "============================================================\n",
      "| Set | Left Image | Right Image |\n",
      "------------------------------------------------------------\n",
      "| Set 1 | 4.7169 | 5.6019 |\n",
      "  → Right image scores higher by 0.8850 points\n",
      "------------------------------------------------------------\n",
      "| Set 2 | 4.1938 | 5.8174 |\n",
      "  → Right image scores higher by 1.6236 points\n",
      "------------------------------------------------------------\n",
      "| Set 3 | 5.7389 | 4.1136 |\n",
      "  → Left image scores higher by 1.6253 points\n",
      "------------------------------------------------------------\n",
      "| Set 4 | 6.8118 | 5.6914 |\n",
      "  → Left image scores higher by 1.1204 points\n",
      "------------------------------------------------------------\n",
      "| Set 5 | 5.2023 | 4.4697 |\n",
      "  → Left image scores higher by 0.7325 points\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Section 1: Process Left/Right image sets\n",
    "print(\"SECTION 1: Aesthetic Scores for Left/Right Image Sets\")\n",
    "print(\"=\" * 60)\n",
    "print(\"| Set | Left Image | Right Image |\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for set_num in range(1, 6):  # Sets 1-5\n",
    "    left_image = f\"PhotoSet/set{set_num}L.jpg\"\n",
    "    right_image = f\"PhotoSet/set{set_num}R.jpg\"\n",
    "    \n",
    "    # Get scores\n",
    "    left_score = get_aesthetic_score(left_image)\n",
    "    right_score = get_aesthetic_score(right_image)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"| Set {set_num} | {left_score:.4f} | {right_score:.4f} |\")\n",
    "    \n",
    "    # Optional: indicate which image has higher aesthetic score\n",
    "    if left_score is not None and right_score is not None:\n",
    "        if left_score > right_score:\n",
    "            print(f\"  → Left image scores higher by {left_score - right_score:.4f} points\")\n",
    "        elif right_score > left_score:\n",
    "            print(f\"  → Right image scores higher by {right_score - left_score:.4f} points\")\n",
    "        else:\n",
    "            print(\"  → Both images have equal scores\")\n",
    "    \n",
    "    print(\"-\" * 60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "SECTION 2: Aesthetic Scores for Multi-Image Sets\n",
      "================================================================================\n",
      "Multi-Set 1 (Total Images: 5)\n",
      "--------------------------------------------------------------------------------\n",
      "| Image | Filename | Aesthetic Score |\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Section 2: Process Multi-image sets\n",
    "print(\"\\n\\nSECTION 2: Aesthetic Scores for Multi-Image Sets\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Process multi-image sets\n",
    "# Format: multiSet{set_num}_{total_images}_{image_index}.jpg\n",
    "# Example: multiSet1_5_1.jpg means set 1, 5 total images, image 1\n",
    "\n",
    "# Define the multi-sets to process\n",
    "multi_sets = [\n",
    "    {\"set_num\": 1, \"total_images\": 5}\n",
    "    # Add more multi-sets here if needed\n",
    "]\n",
    "\n",
    "for multi_set in multi_sets:\n",
    "    set_num = multi_set[\"set_num\"]\n",
    "    total_images = multi_set[\"total_images\"]\n",
    "    \n",
    "    print(f\"Multi-Set {set_num} (Total Images: {total_images})\")\n",
    "    print(\"-\" * 80)\n",
    "    print(\"| Image | Filename | Aesthetic Score |\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    scores = []\n",
    "    for img_idx in range(1, total_images + 1):\n",
    "        img_filename = f\"PhotoSet/multiSet{set_num}_{total_images}_{img_idx}.jpg\"\n",
    "        \n",
    "        # Get score for this image\n",
    "        score = get_aesthetic_score(img_filename)\n",
    "        scores.append(score)\n",
    "        \n",
    "        # Print individual image score\n",
    "        print(f\"| {img_idx} | {img_filename} | {score:.4f} |\")\n",
    "    \n",
    "    # Print summary statistics\n",
    "    if scores and all(s is not None for s in scores):\n",
    "        scores = np.array(scores)\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Summary Statistics for Multi-Set {set_num}:\")\n",
    "        print(f\"  → Average Score: {np.mean(scores):.4f}\")\n",
    "        print(f\"  → Median Score: {np.median(scores):.4f}\")\n",
    "        print(f\"  → Min Score: {np.min(scores):.4f} (Image {np.argmin(scores) + 1})\")\n",
    "        print(f\"  → Max Score: {np.max(scores):.4f} (Image {np.argmax(scores) + 1})\")\n",
    "        print(f\"  → Standard Deviation: {np.std(scores):.4f}\")\n",
    "    \n",
    "    print(\"=\" * 80)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "asthetics_predictor.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.11 (default)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
